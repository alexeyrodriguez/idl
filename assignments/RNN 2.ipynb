{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (7,7) # Make the figures a bit bigger\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.rnn.python.ops import rnn_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  9.8M  100  9.8M    0     0  36.7M      0 --:--:-- --:--:-- --:--:-- 36.6M\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ../data\n",
    "!curl -L -o ../data/will_play_text.csv https://commondatastorage.googleapis.com/ckannet-storage/2012-04-24T183403/will_play_text.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/will_play_text.csv', sep=';', header=None, names=['row', 'piece', 'chapter', 'section', 'character', 'line'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(111005, 66)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = df[~df.character.isnull()].line\n",
    "\n",
    "# mapping\n",
    "letters = ['>', '<'] + list(set(''.join(list(sentences))))\n",
    "nletters = len(letters)\n",
    "\n",
    "bw_mapping = dict(enumerate(letters, 1))\n",
    "fw_mapping = dict([(c, ix) for ix, c in enumerate(letters, 1)])\n",
    "\n",
    "rsen_sz = 64\n",
    "sen_sz = 1 + rsen_sz + 1\n",
    "\n",
    "def pad_sentence(s):\n",
    "    l = len(s)\n",
    "    coded = [fw_mapping[c] for c in ['>'] + list(s) + ['<']]\n",
    "    return coded + [0] * (rsen_sz - l)\n",
    "\n",
    "# dataset\n",
    "sentences = [pad_sentence(s) for s in sentences if len(s)<=rsen_sz]\n",
    "sentences = np.array(sentences)\n",
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 29, 14, 21, 58, 41, 70, 59, 53, 54, 22, 59, 58, 11, 22, 10, 70,\n",
       "       54, 71,  8, 36, 14, 21, 59, 21, 58, 53, 67, 67, 59, 14, 54, 11, 58,\n",
       "       10, 13, 58, 14, 59,  8, 58, 41, 70, 10, 36, 64, 11,  2,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state_size = 256\n",
    "\n",
    "def rnn_text(sen_sz=sen_sz):\n",
    "    cell_state = tf.placeholder(tf.float32, [batch_size, hidden_state_size])\n",
    "    hidden_state = tf.placeholder(tf.float32, [batch_size, hidden_state_size])\n",
    "    init_state = tf.nn.rnn_cell.LSTMStateTuple(cell_state, hidden_state)\n",
    "    \n",
    "    seq_input = tf.placeholder(tf.int64, [None, sen_sz], name='sentence_input')\n",
    "    \n",
    "    xs = seq_input[:, :-1]\n",
    "    ys = seq_input[:, 1:]\n",
    "    \n",
    "    xs_valid_mask = tf.logical_and(tf.not_equal(xs, 0), tf.not_equal(xs, 2)) # exclude last character\n",
    "    xs_seq_len = tf.reduce_sum(tf.to_int32(xs_valid_mask), axis=1)\n",
    "    \n",
    "    # Turn to zero-based\n",
    "    #xs = tf.Print(xs, [xs], message='xs before', summarize=100)\n",
    "    #xs = xs - 1\n",
    "    #xs = tf.Print(xs, [xs], message='xs after', summarize=100)\n",
    "    #ys = ys - 1\n",
    "    #xs = tf.Print(xs, [ys], message='ys after', summarize=100)\n",
    "    #xs = tf.Print(xs, [xs_seq_len], message='xs length', summarize=100)\n",
    "    #xs = tf.Print(xs, [xs_valid_mask], message='xs mask', summarize=100)\n",
    "    #xs = tf.Print(xs, [xs[:, xs_seq_len-1]], message='last xs', summarize=100)\n",
    "    #xs = tf.Print(xs, [xs_seq_len, xs[:, xs_seq_len-1]], message='xs length', summarize=100)\n",
    "    \n",
    "    xs_one_hot = tf.one_hot(xs, depth=nletters+1)\n",
    "    ys_one_hot = tf.one_hot(ys, depth=nletters+1)\n",
    "    \n",
    "    rnn_cell = tf.nn.rnn_cell.LSTMCell(hidden_state_size, state_is_tuple=True)\n",
    "    outputs, state = tf.nn.dynamic_rnn(rnn_cell, xs_one_hot, sequence_length=xs_seq_len, initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope(\"output\"):\n",
    "        W_hy = tf.get_variable('W_hy', [hidden_state_size, nletters+1])\n",
    "        B_hy = tf.get_variable('B_hy', [nletters+1])\n",
    "    \n",
    "    logits = tf.tensordot(outputs, W_hy, axes=1) + B_hy\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=ys_one_hot, logits=logits)\n",
    "    masked_cross_entropy = tf.multiply(cross_entropy, tf.to_float(xs_valid_mask))\n",
    "    summed_entropy = tf.reduce_sum(masked_cross_entropy, 1)\n",
    "    sequence_entropy = tf.divide(summed_entropy, tf.to_float(xs_seq_len))\n",
    "    total_entropy = tf.reduce_mean(sequence_entropy)\n",
    "    \n",
    "    return seq_input, (cell_state, hidden_state), total_entropy\n",
    "\n",
    "\n",
    "def rnn_synth_text(sen_sz=sen_sz):\n",
    "    cell_state = tf.placeholder(tf.float32, [batch_size, hidden_state_size])\n",
    "    hidden_state = tf.placeholder(tf.float32, [batch_size, hidden_state_size])\n",
    "    init_state = tf.nn.rnn_cell.LSTMStateTuple(cell_state, hidden_state)\n",
    "    \n",
    "    char_input = tf.placeholder(tf.int64, [None])\n",
    "    x = tf.one_hot(char_input, depth=nletters)\n",
    "    \n",
    "    rnn_cell = tf.nn.rnn_cell.LSTMCell(hidden_state_size, state_is_tuple=True)\n",
    "    outputs, state = tf.nn.dynamic_rnn(rnn_cell, x, sequence_length=[1], initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope(\"output\"):\n",
    "        W_hy = tf.get_variable('W_hy', [hidden_state_size, nletters])\n",
    "        B_hy = tf.get_variable('B_hy', [nletters])\n",
    "\n",
    "    logits = tf.tensordot(outputs, W_hy, axes=1) + B_hy\n",
    "    probs = tf.nn.softmax(logits=logits)\n",
    "    \n",
    "    return char_input, init_state, probs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_optimize(sess, train, Xs, loss,\n",
    "                name='train', steps=1000,\n",
    "                optimizer=tf.train.GradientDescentOptimizer(0.5),\n",
    "                dict={}, batch_size=100, report_steps=500):\n",
    "    train_step = optimizer.minimize(loss)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(steps):\n",
    "        \n",
    "        # Such a hack\n",
    "        start = (i * batch_size) % train.shape[0]\n",
    "        end = ((i+1) * batch_size) % train.shape[0]\n",
    "        \n",
    "        if start < end:\n",
    "            batch_xs = train[start:end]\n",
    "            dict = dict.copy()# Avoid getting the keys from a different graph\n",
    "            dict[Xs] = batch_xs\n",
    "            _, xloss = sess.run([train_step, loss], feed_dict=dict)\n",
    "            \n",
    "            if i % report_steps == 0:\n",
    "                print('Step {}\\tLoss: {}'.format(i, xloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\tLoss: 4.405694484710693\n",
      "Step 100\tLoss: 3.079648971557617\n",
      "Step 200\tLoss: 2.7579002380371094\n",
      "Step 300\tLoss: 2.5285120010375977\n",
      "Step 400\tLoss: 2.381396532058716\n",
      "Step 500\tLoss: 2.2919301986694336\n",
      "Step 600\tLoss: 2.2863636016845703\n",
      "Step 700\tLoss: 2.2690131664276123\n",
      "Step 800\tLoss: 2.2186548709869385\n",
      "Step 900\tLoss: 2.224323272705078\n",
      "Step 1000\tLoss: 2.140319585800171\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        Xs, (cell_state, hidden_state), loss = rnn_text()\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        my_optimize(sess,\n",
    "                    sentences,\n",
    "                    Xs,\n",
    "                    loss,\n",
    "                    steps=1001,\n",
    "                    report_steps=100,\n",
    "                    name='train-1',\n",
    "                    batch_size=batch_size,\n",
    "                    dict={cell_state: np.zeros((batch_size, hidden_state_size)),\n",
    "                          hidden_state: np.zeros((batch_size, hidden_state_size)),\n",
    "                         },\n",
    "                    optimizer=optimizer)\n",
    "        tf.train.Saver().save(sess, 'artifacts/model-rnn-2-v1.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from artifacts/model-rnn-2-v1.ckpt\n",
      ">the gotone aly epritut opourel grons awf the aysure.<\n"
     ]
    }
   ],
   "source": [
    "def rnn_synth_text(sen_sz=sen_sz):\n",
    "    cell_state = tf.placeholder(tf.float32, [batch_size, hidden_state_size])\n",
    "    hidden_state = tf.placeholder(tf.float32, [batch_size, hidden_state_size])\n",
    "    init_state = tf.nn.rnn_cell.LSTMStateTuple(cell_state, hidden_state)\n",
    "    \n",
    "    char_input = tf.placeholder(tf.int64, [None, None], name='input_char')\n",
    "    # turn to zero-based\n",
    "    #char_input = char_input - 1\n",
    "    x = tf.one_hot(char_input, depth=nletters+1)\n",
    "    \n",
    "    rnn_cell = tf.nn.rnn_cell.LSTMCell(hidden_state_size, state_is_tuple=True)\n",
    "    outputs, state = tf.nn.dynamic_rnn(rnn_cell, x, sequence_length=[1], initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope(\"output\"):\n",
    "        W_hy = tf.get_variable('W_hy', [hidden_state_size, nletters+1])\n",
    "        B_hy = tf.get_variable('B_hy', [nletters+1])\n",
    "\n",
    "    logits = tf.tensordot(outputs, W_hy, axes=1) + B_hy\n",
    "    probs = tf.nn.softmax(logits=logits)\n",
    "    \n",
    "    return char_input, (cell_state, hidden_state), probs, state\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        char_input, init_state, p_output, out_state = rnn_synth_text()\n",
    "        \n",
    "        tf.train.Saver().restore(sess, 'artifacts/model-rnn-2-v1.ckpt')\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        res = ['>']\n",
    "        cur_char = fw_mapping['>']\n",
    "        \n",
    "        cur_cell_state = np.zeros((batch_size, hidden_state_size))\n",
    "        cur_hidden_state = np.zeros((batch_size, hidden_state_size))\n",
    "\n",
    "        for _ in range(100):#sen_sz - 1):        \n",
    "            ps, state = sess.run([p_output, out_state],\n",
    "                                 feed_dict={char_input: np.array([cur_char]).reshape((1, 1)),\n",
    "                                            init_state: (cur_cell_state, cur_hidden_state)})\n",
    "            #cur_char = np.argmax(ps)\n",
    "            cur_char = np.random.choice(nletters+1, p=ps.reshape(nletters+1))\n",
    "            # Turn to one-based\n",
    "            #cur_char = cur_char + 1\n",
    "            cur_state = state\n",
    "            res.append(bw_mapping[cur_char])\n",
    "            \n",
    "            cur_cell_state, cur_hidden_state = state\n",
    "            \n",
    "            if bw_mapping[cur_char] is '<':\n",
    "                break\n",
    "        \n",
    "        print (''.join(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
