{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (7,7) # Make the figures a bit bigger\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  9.8M  100  9.8M    0     0  5772k      0  0:00:01  0:00:01 --:--:-- 5770k\n"
     ]
    }
   ],
   "source": [
    "!curl -L -o ../data/will_play_text.csv https://commondatastorage.googleapis.com/ckannet-storage/2012-04-24T183403/will_play_text.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "รง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row</th>\n",
       "      <th>piece</th>\n",
       "      <th>chapter</th>\n",
       "      <th>section</th>\n",
       "      <th>character</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Henry IV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ACT I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Henry IV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SCENE I. London. The palace.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Henry IV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Enter KING HENRY, LORD JOHN OF LANCASTER, the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Henry IV</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1.1</td>\n",
       "      <td>KING HENRY IV</td>\n",
       "      <td>So shaken as we are, so wan with care,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Henry IV</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1.2</td>\n",
       "      <td>KING HENRY IV</td>\n",
       "      <td>Find we a time for frighted peace to pant,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Henry IV</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1.3</td>\n",
       "      <td>KING HENRY IV</td>\n",
       "      <td>And breathe short-winded accents of new broils</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Henry IV</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1.4</td>\n",
       "      <td>KING HENRY IV</td>\n",
       "      <td>To be commenced in strands afar remote.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Henry IV</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1.5</td>\n",
       "      <td>KING HENRY IV</td>\n",
       "      <td>No more the thirsty entrance of this soil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Henry IV</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1.6</td>\n",
       "      <td>KING HENRY IV</td>\n",
       "      <td>Shall daub her lips with her own children's bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Henry IV</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1.7</td>\n",
       "      <td>KING HENRY IV</td>\n",
       "      <td>Nor more shall trenching war channel her fields,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row     piece  chapter section      character  \\\n",
       "0    1  Henry IV      NaN     NaN            NaN   \n",
       "1    2  Henry IV      NaN     NaN            NaN   \n",
       "2    3  Henry IV      NaN     NaN            NaN   \n",
       "3    4  Henry IV      1.0   1.1.1  KING HENRY IV   \n",
       "4    5  Henry IV      1.0   1.1.2  KING HENRY IV   \n",
       "5    6  Henry IV      1.0   1.1.3  KING HENRY IV   \n",
       "6    7  Henry IV      1.0   1.1.4  KING HENRY IV   \n",
       "7    8  Henry IV      1.0   1.1.5  KING HENRY IV   \n",
       "8    9  Henry IV      1.0   1.1.6  KING HENRY IV   \n",
       "9   10  Henry IV      1.0   1.1.7  KING HENRY IV   \n",
       "\n",
       "                                                line  \n",
       "0                                              ACT I  \n",
       "1                       SCENE I. London. The palace.  \n",
       "2  Enter KING HENRY, LORD JOHN OF LANCASTER, the ...  \n",
       "3             So shaken as we are, so wan with care,  \n",
       "4         Find we a time for frighted peace to pant,  \n",
       "5     And breathe short-winded accents of new broils  \n",
       "6            To be commenced in strands afar remote.  \n",
       "7          No more the thirsty entrance of this soil  \n",
       "8  Shall daub her lips with her own children's bl...  \n",
       "9   Nor more shall trenching war channel her fields,  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87336, 31)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = df[~df.character.isnull()].line\n",
    "\n",
    "# mapping\n",
    "letters = ['>'] + list(set(''.join(list(sentences))))\n",
    "nletters = len(letters)\n",
    "\n",
    "bw_mapping = dict(enumerate(letters))\n",
    "fw_mapping = dict([(c, ix) for ix, c in enumerate(letters)])\n",
    "\n",
    "sen_sz = 1 + 30\n",
    "\n",
    "# dataset\n",
    "sentences = [[fw_mapping[c] for c in ['>'] + list(s[:sen_sz - 1])] for s in sentences if len(s)>(sen_sz - 1)]\n",
    "sentences = np.array(sentences)\n",
    "sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['>',\n",
       " 'F',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " 'm',\n",
       " 'o',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'b',\n",
       " 'e',\n",
       " ' ',\n",
       " 's',\n",
       " 'a',\n",
       " 'i',\n",
       " 'd',\n",
       " ' ',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[bw_mapping[c] for c in list(sentences[102])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def variable_summaries(var):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)\n",
    "\n",
    "# From Aylien's blog https://github.com/AYLIEN/gan-intro/blob/master/gan.py\n",
    "def linear(input, output_dim, scope=None, stddev=1.0):\n",
    "    with tf.variable_scope(scope or 'linear'):\n",
    "        with tf.variable_scope('weights'):\n",
    "            w = tf.get_variable(\n",
    "                'w',\n",
    "                [input.get_shape()[1], output_dim]#,\n",
    "                #initializer=tf.random_normal_initializer(stddev=stddev)\n",
    "            )\n",
    "            variable_summaries(w)\n",
    "        with tf.variable_scope('bias'):\n",
    "            b = tf.get_variable(\n",
    "                'b',\n",
    "                [output_dim]#,\n",
    "                #initializer=tf.constant_initializer(0.5)\n",
    "            )\n",
    "            variable_summaries(b)\n",
    "        return tf.matmul(input, w) + b, (w, b)\n",
    "\n",
    "def bilinear(input, state, output_dim, scope=None, stddev=1.0):\n",
    "    with tf.variable_scope(scope or 'linear'):\n",
    "        with tf.variable_scope('input_weights'):\n",
    "            iw = tf.get_variable(\n",
    "                 'iw',\n",
    "                 [input.get_shape()[1], output_dim]#,\n",
    "                 #initializer=tf.random_normal_initializer(stddev=stddev)\n",
    "            )\n",
    "            variable_summaries(iw)\n",
    "        with tf.variable_scope('state_weights'):\n",
    "            sw = tf.get_variable(\n",
    "                 'sw',\n",
    "                 [state.get_shape()[1], output_dim]#,\n",
    "                 #initializer=tf.random_normal_initializer(stddev=stddev)\n",
    "            )\n",
    "            variable_summaries(sw)\n",
    "        with tf.variable_scope('bias'):\n",
    "            b = tf.get_variable(\n",
    "                'b',\n",
    "                [output_dim]#,\n",
    "                #initializer=tf.constant_initializer(0.5)\n",
    "            )\n",
    "            variable_summaries(b)\n",
    "        return tf.matmul(input, iw) + tf.matmul(state, sw) + b, (iw, sw, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_optimize(sess, train, Xs, loss,\n",
    "                name='train', steps=1000,\n",
    "                optimizer=tf.train.GradientDescentOptimizer(0.5),\n",
    "                dict={}, batch_size=100, report_steps=500):\n",
    "    train_step = optimizer.minimize(loss)\n",
    "    \n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter('rnn_logs/%s' % name, sess.graph)\n",
    "    \n",
    "    for g, v in optimizer.compute_gradients(loss):\n",
    "        #tf.summary.scalar(\"grad_{}\".format(v.name), tf.norm(g))\n",
    "        #merged = tf.Print(merged, [tf.norm(g)], message=\"grad_{}\".format(v.name))\n",
    "        pass\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(steps):\n",
    "        \n",
    "        # Such a hack\n",
    "        start = (i * batch_size) % train.shape[0]\n",
    "        end = ((i+1) * batch_size) % train.shape[0]\n",
    "        \n",
    "        if start < end:\n",
    "            batch_xs = train[start:end]\n",
    "            dict = dict.copy()# Avoid getting the keys from a different graph\n",
    "            dict[Xs] = batch_xs\n",
    "            summary, _, xloss = sess.run([merged, train_step, loss], feed_dict=dict)\n",
    "            \n",
    "            if i % report_steps == 0:\n",
    "                print('Step {}\\tLoss: {}'.format(i, xloss))\n",
    "\n",
    "            train_writer.add_summary(summary, i)\n",
    "        \n",
    "    train_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_state_size = 512\n",
    "\n",
    "def rnn_text(sen_sz=31):\n",
    "\n",
    "    init_state = tf.placeholder(tf.float32, [None, hidden_state_size])\n",
    "    hidden_state = init_state\n",
    "\n",
    "    seq_input = tf.placeholder(tf.int64, [None, sen_sz])\n",
    "    enc_seq_input = tf.one_hot(seq_input, nletters) # adds a trailing dimension with the hot encoding\n",
    "\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(sen_sz-1):\n",
    "        x = enc_seq_input[:, i, :]\n",
    "        y = enc_seq_input[:, i+1, :]\n",
    "        \n",
    "        #x = tf.Print(x, [i, tf.argmax(tf.reshape(x, [nletters])), tf.argmax(tf.reshape(y, [nletters]))])\n",
    "        \n",
    "        with tf.variable_scope('step', reuse=i!=0):\n",
    "            hidden_state, _ = bilinear(x, hidden_state, hidden_state_size, 'hl1')\n",
    "            hidden_state = tf.nn.tanh(hidden_state)\n",
    "            output, _ = linear(hidden_state, nletters, 'ol1')\n",
    "\n",
    "        costs.append(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output))\n",
    "        \n",
    "    loss = tf.reduce_mean(costs)\n",
    "    \n",
    "    return seq_input, init_state, loss\n",
    "\n",
    "def rnn_synth_text(sen_sz=31):\n",
    "\n",
    "    init_state = tf.placeholder(tf.float32, [None, hidden_state_size])\n",
    "    hidden_state = init_state\n",
    "\n",
    "    char_input = tf.placeholder(tf.int64, [None])\n",
    "    x = tf.one_hot(char_input, nletters)\n",
    "    \n",
    "    with tf.variable_scope('step'):\n",
    "        hidden_state, _ = bilinear(x, hidden_state, hidden_state_size, 'hl1')\n",
    "        hidden_state = tf.nn.tanh(hidden_state)\n",
    "        output, _ = linear(hidden_state, nletters, 'ol1')\n",
    "\n",
    "    py = tf.nn.softmax(output)\n",
    "    \n",
    "    return char_input, init_state, py, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\tLoss: 4.440620422363281\n",
      "Step 500\tLoss: 2.117955446243286\n",
      "Step 1000\tLoss: 1.9799425601959229\n",
      "Step 1500\tLoss: 1.8005975484848022\n",
      "Step 2000\tLoss: 1.6870160102844238\n",
      "Step 2500\tLoss: 1.6520211696624756\n",
      "Step 3000\tLoss: 1.6153658628463745\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        Xs, init_state, loss = rnn_text()\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        my_optimize(sess,\n",
    "                    sentences,\n",
    "                    Xs,\n",
    "                    loss,\n",
    "                    steps=3001,\n",
    "                    report_steps=500,\n",
    "                    name='train-1',\n",
    "                    batch_size=batch_size,\n",
    "                    dict={init_state: np.zeros((batch_size, hidden_state_size))},\n",
    "                    optimizer=optimizer)\n",
    "        \n",
    "        tf.train.Saver().save(sess, 'artifacts/model-rnn-v2-1.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from artifacts/model-rnn-v2-1.ckpt\n",
      ">Call my tyou and tell her see is bedstries, 'Tis no demplous not quarren in my lond is she is saiding to dornous gread and neary inmmon's part, weak make pointed, here creeclest king tonesture as I dar, you did nighted; I despeting dore thy didilike cerague, ceede love, nor bigs from ancester! Where's no disters, revence to be frort'dlad on a mindred, I'll me tew proches, a will be mucted they cot love! I west aulied, I bester them, lords! O: when he to you actier pray that till Become, from hep\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        char_input, init_state, p_output, out_state = rnn_synth_text()\n",
    "        \n",
    "        tf.train.Saver().restore(sess, 'artifacts/model-rnn-v2-1.ckpt')\n",
    "        #sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        res = ['>']\n",
    "        cur_char = fw_mapping['>']\n",
    "        cur_state = np.zeros((batch_size, hidden_state_size))\n",
    "\n",
    "        for _ in range(500):#sen_sz - 1):        \n",
    "            ps, state = sess.run([p_output, out_state],\n",
    "                                 feed_dict={char_input: np.array([cur_char]), init_state: cur_state})\n",
    "            #cur_char = np.argmax(ps)\n",
    "            cur_char = np.random.choice(nletters, p=ps.reshape(nletters))\n",
    "            cur_state = state\n",
    "            res.append(bw_mapping[cur_char])\n",
    "        \n",
    "        print (''.join(res))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second part, using longer sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('../data/shakespear.txt').read()\n",
    "\n",
    "# mapping\n",
    "letters = ['>'] + list(set(text))\n",
    "nletters = len(letters)\n",
    "\n",
    "bw_mapping = dict(enumerate(letters))\n",
    "fw_mapping = dict([(c, ix) for ix, c in enumerate(letters)])\n",
    "\n",
    "base_sz = 50\n",
    "sen_sz = 1 + base_sz\n",
    "\n",
    "# dataset\n",
    "sentences = ['>' + text[i:i+base_sz] for i in range(0, len(text), base_sz)]\n",
    "sentences = [[fw_mapping[c] for c in s] for s in sentences if len(s)==sen_sz]\n",
    "sentences = np.array(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_state_size = 512\n",
    "\n",
    "def rnn_text(sen_sz=31):\n",
    "\n",
    "    init_state = tf.placeholder(tf.float32, [None, hidden_state_size])\n",
    "    hidden_state = init_state\n",
    "\n",
    "    seq_input = tf.placeholder(tf.int64, [None, sen_sz])\n",
    "    enc_seq_input = tf.one_hot(seq_input, nletters) # adds a trailing dimension with the hot encoding\n",
    "\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(sen_sz-1):\n",
    "        x = enc_seq_input[:, i, :]\n",
    "        y = enc_seq_input[:, i+1, :]\n",
    "        \n",
    "        #x = tf.Print(x, [i, tf.argmax(tf.reshape(x, [nletters])), tf.argmax(tf.reshape(y, [nletters]))])\n",
    "        \n",
    "        with tf.variable_scope('step', reuse=i!=0):\n",
    "            hidden_state, _ = bilinear(x, hidden_state, hidden_state_size, 'hl1')\n",
    "            hidden_state = tf.nn.tanh(hidden_state)\n",
    "            output, _ = linear(hidden_state, nletters, 'ol1')\n",
    "\n",
    "        costs.append(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output))\n",
    "        \n",
    "    loss = tf.reduce_mean(costs)\n",
    "    \n",
    "    return seq_input, init_state, loss\n",
    "\n",
    "def rnn_synth_text(sen_sz=31):\n",
    "\n",
    "    init_state = tf.placeholder(tf.float32, [None, hidden_state_size])\n",
    "    hidden_state = init_state\n",
    "\n",
    "    char_input = tf.placeholder(tf.int64, [None])\n",
    "    x = tf.one_hot(char_input, nletters)\n",
    "    \n",
    "    with tf.variable_scope('step'):\n",
    "        hidden_state, _ = bilinear(x, hidden_state, hidden_state_size, 'hl1')\n",
    "        hidden_state = tf.nn.tanh(hidden_state)\n",
    "        output, _ = linear(hidden_state, nletters, 'ol1')\n",
    "\n",
    "    py = tf.nn.softmax(output)\n",
    "    \n",
    "    return char_input, init_state, py, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\tLoss: 4.191850662231445\n",
      "Step 500\tLoss: 2.0128650665283203\n",
      "Step 1000\tLoss: 1.5846800804138184\n",
      "Step 1500\tLoss: 1.3026831150054932\n",
      "Step 2000\tLoss: 1.0218708515167236\n",
      "Step 2500\tLoss: 0.8547126054763794\n",
      "Step 3000\tLoss: 0.7481168508529663\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        Xs, init_state, loss = rnn_text(sen_sz=sen_sz)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        my_optimize(sess,\n",
    "                    sentences,\n",
    "                    Xs,\n",
    "                    loss,\n",
    "                    steps=3001,\n",
    "                    report_steps=500,\n",
    "                    name='train-1',\n",
    "                    batch_size=batch_size,\n",
    "                    dict={init_state: np.zeros((batch_size, hidden_state_size))},\n",
    "                    optimizer=optimizer)\n",
    "        \n",
    "        tf.train.Saver().save(sess, 'artifacts/model-rnn-shakespeare-200.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from artifacts/model-rnn-shakespeare-200.ckpt\n",
      ">ther thee\n",
      "thal maillve mull rovech merness, tie pervait you f lleat you thel.\n",
      "\n",
      "Th! Thoul\n",
      "Tie thom at reance wet than se\n",
      "Be ngain you tove: a will.\n",
      "\n",
      "MINGA:\n",
      "Pedisin ers reck ciptarus\n",
      "no ertiof e wilt R\n",
      "seles, and to, that fflones? hat yom seall, me tervaryous wat grom that\n",
      "Is maknow tod'thingutrevill you, lyod I lave noll enveppreity und ler isanits\n",
      "Ged Is mest you Ches lave me thaiflisea lave me tarese\n",
      "me lids ay rivet\n",
      "\n",
      "Kser llod is, st mens el furd ne, hid Sif lee boaks nos far whot wet he whe, \n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        char_input, init_state, p_output, out_state = rnn_synth_text(sen_sz=sen_sz)\n",
    "        \n",
    "        tf.train.Saver().restore(sess, 'artifacts/model-rnn-shakespeare-200.ckpt')\n",
    "        \n",
    "        res = ['>']\n",
    "        cur_char = fw_mapping['>']\n",
    "        cur_state = np.zeros((batch_size, hidden_state_size))\n",
    "\n",
    "        for _ in range(500):#sen_sz - 1):        \n",
    "            ps, state = sess.run([p_output, out_state],\n",
    "                                 feed_dict={char_input: np.array([cur_char]), init_state: cur_state})\n",
    "            #cur_char = np.argmax(ps)\n",
    "            cur_char = np.random.choice(nletters, p=ps.reshape(nletters))\n",
    "            cur_state = state\n",
    "            res.append(bw_mapping[cur_char])\n",
    "        \n",
    "        print (''.join(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
